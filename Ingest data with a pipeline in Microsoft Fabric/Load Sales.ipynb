{"cells":[{"cell_type":"code","source":["table_name = 'sales2'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"9fb90e24-b61b-44b9-95bd-38eb7ad58876"},{"cell_type":"markdown","source":["In the **â€¦** menu for the above cell (at its top-right) select **Toggle parameter cell**. This configures the cell so that <mark>the variables declared in it are treated as parameters when running the notebook from a pipeline</mark>."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"60daa267-44a0-4d31-8b6b-1220c434afd4"},{"cell_type":"code","source":["from pyspark.sql.functions import *\n","\n","# Read the new sales data \n","df = spark.read.format('csv').option('header', 'true').load('Files/new_data/*.csv')\n","\n","# Add month and year columns\n","df = df.withColumn('Year', year(col('OrderDate'))).withColumn('Month', month(col('OrderDate')))\n","\n","# Derive FirstName and LastName columns\n","df = df.withColumn('FirstName', split(col('CustomerName'), \" \").getItem(0)).withColumn('lastName', split(col('CustomerName'), \" \").getItem(1))\n","\n","# Filter and reorder columns\n","df = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n","\n","# Load the data into a table\n","df.write.format('delta').mode('append').saveAsTable(table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"ed3ca344-6764-41fd-a1c8-449ceac5ebc0","normalized_state":"finished","queued_time":"2025-08-24T07:53:45.9837646Z","session_start_time":null,"execution_start_time":"2025-08-24T07:53:46.2004585Z","execution_finish_time":"2025-08-24T07:54:00.1289255Z","parent_msg_id":"e89e8c46-7d6f-4899-95a9-9d103173d18a"},"text/plain":"StatementMeta(, ed3ca344-6764-41fd-a1c8-449ceac5ebc0, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62177724-faa9-473c-a927-f72a7d391efa"},{"cell_type":"markdown","source":["##### Implemented a data ingestion solution that uses a pipeline to copy data to your lakehouse from an external source, and then uses a Spark notebook to transform the data and load it into a table."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e91ce02-4a7b-4811-a35c-cda6e7770b9f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"version":"0.1","state":{}},"dependencies":{"lakehouse":{"default_lakehouse":"542b7a1d-d0d6-4e33-a393-ee3cffa2970e","known_lakehouses":[{"id":"542b7a1d-d0d6-4e33-a393-ee3cffa2970e"}],"default_lakehouse_name":"lakehouse_store_bigData","default_lakehouse_workspace_id":"376436a8-fa6c-4f70-8116-2f3c3f3a3a64"}}},"nbformat":4,"nbformat_minor":5}